# Préparation d'un dossier qui va contenir les données sur hdfs  
hdfs dfs -ls
hdfs dfs -mkdir /user/cloudera/datatp3
hdfs dfs -mkdir  /user/cloudera/datatp3/football
#Récupération des données Football Event depuis Kaggle
wget https://www.kaggle.com/secareanualin/football-events/downloads/football-events.zip
#Exraire les donnesé 
unzip football-events.zip
#Copier les données depuis local Vers hdfs
hdfs dfs -copyFromLocal ginf.csv events.csv dictionary.txt /user/cloudera/datatp3/football
# Creation de schema ou base de données
hive
CREATE DATABASE IF NOT EXISTS football;
SHOW DATABASES;
# Creation des tables sur hive
# Création de table events
use football;
create table events (
id_odsp string, 
id_event string,
sort_order int,
time int,
text string,
event_type int,
event_type2 int,
side int, 
event_team string,
opponent string,
player string,
player2 string,
player_in string, 
player_out string,
shot_place int,
shot_outcome int,
is_goal int,
location int,
bodypart int,
assist_method int,
situation int,
fast_break int
)
row format delimited fields terminated by ','
;

#Chargement de données vers Hive 
#Chargement table vers des tables de staging sur un schema foot 
CREATE SCHEMA IF NOT EXISTS foot;
CREATE EXTERNAL TABLE IF NOT EXISTS foot.events (
id_odsp string, 
id_event string,
sort_order int,
time int,
text string,
event_type int,
event_type2 int,
side int, 
event_team string,
opponent string,
player string,
player2 string,
player_in string, 
player_out string,
shot_place int,
shot_outcome int,
is_goal int,
location int,
bodypart int,
assist_method int,
situation int,
fast_break int
)
 ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ','
 STORED AS TEXTFILE
LOCATION '/user/cloudera/datatp3/football/';
# Vérification que les données sont bien chargé sur les tables de staging
SELECT * FROM foot.events SORT BY id_odsp  DESC LIMIT 2;
#Chargment des données vers la bd football
INSERT INTO TABLE football.events SELECT * FROM foot.events;
SELECT * FROM football.events SORT BY id_odsp  DESC LIMIT 2;
#LOAD DATA INPATH '/user/cloudera/datatp3/football/events.csv' OVERWRITE INTO TABLE events;
LOAD DATA LOCAL INPATH '/home/user/cloudera/.staging/events.csv' OVERWRITE INTO TABLE football.events;
# Vefication de nombre d'enrigistrement avec count
hive> select count(id_odsp) from events;
#Supprimer les données de hdfs
#hdfs dfs -rm /user/cloudera/datatp3/football/events.csv

# Traitement des données 
/********
11 unique events (1-Attempt(shot), 2-Corner, 3-Foul, 4-Yellow Card, 5-Second yellow card, 6-(Straight) red card, 7-Substitution, 8-Free kick won, 9-Offside, 10-Hand Ball, 11-Penalty conceded)
*/
select 
side
, event_team
, time
, text
from events
where event_type=11;
/*********/

/********Home teams get more penalty kicks than visitors?*******/
select 
count(side) as side
from events
where event_type=11
and text like '%draws%'
group by side
;
/*****************************************/

#chargement de table ginf
# Création de table ginf,
use football; 
create table if not exists football.ginf (
 id_odsp string,
 link_odsp string,
 adv_stats BOOLEAN,
 date date,
 league string,
 season string,
 country string,
 ht string,
 at  string,
 fthg  int,
 ftag int,
 odd_h DOUBLE,
 odd_d DOUBLE,
 odd_a DOUBLE,
 odd_over DOUBLE,
 odd_under DOUBLE,
 odd_bts DOUBLE,
 odd_bts_n DOUBLE
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/cloudera/datatp3/football/'
;

## Staging 
CREATE EXTERNAL TABLE IF NOT EXISTS foot.ginf (
 id_odsp string,
 link_odsp string,
 adv_stats BOOLEAN,
 date date,
 league string,
 season string,
 country string,
 ht string,
 at  string,
 fthg  int,
 ftag int,
 odd_h DOUBLE,
 odd_d DOUBLE,
 odd_a DOUBLE,
 odd_over DOUBLE,
 odd_under DOUBLE,
 odd_bts DOUBLE,
 odd_bts_n DOUBLE
)
 ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  STORED AS TEXTFILE
 LOCATION '/user/cloudera/datatp3/football/';

SELECT * FROM foot.ginf SORT BY id_odsp  DESC LIMIT 2;
INSERT INTO TABLE football.ginf SELECT * FROM foot.ginf;
hdfs dfs -rm /user/cloudera/datatp3/football/ginf.csv

# El Classico
hive> select * from ginf where country like '%spain%' and ht like '%Barcelona%' and at like '%Real Madrid;
hive> select * from ginf where country like '%spain%' and ht like '%Real Madrid'  and at like '%Barcelona%';

select g.*, ev.*
from football.ginf as g
right join football.events as ev on (g.id_odsp=ev.id_odsp)
where g.country like '%spain%' and  g.ht like '%Barcelona%' and g.at like '%Real Madrid%'; 

# Sentiment Analysis El Classico 
select g.*, ev.text 
from football.ginf as g
right join football.events as ev on (g.id_odsp=ev.id_odsp)
where g.country like '%spain%' and  g.ht like '%Barcelona%' and g.at like '%Real Madrid%'; 


#Load Data to Pig from Hive
#Note: While loading Hive data into Pig relation, make sure that the user has started Hive metastore service using the below command –
sudo service hive-metastore start
hive –service metastore

Keep the Hive metastore service running in one terminal and use Pig in another terminal
Now to load the hive data into pig, Pig uses HCataLoader() function and it looks like this

pig
grunt > ginfpig = LOAD 'football.ginf' USING org.apache.hive.hcatalog.pig.HCatLoader();
grunt > describe ginfpig;

grunt > eventspig = LOAD 'football.events' USING org.apache.hive.hcatalog.pig.HCatLoader();
grunt > describe eventspig;

#El Classico
select g.*, ev.text 
from football.ginf as g
right join football.events as ev on (g.id_odsp=ev.id_odsp)
where g.country like '%spain%' and  g.ht like '%Barcelona%' and g.at like '%Real Madrid%'; 

ElCalssico under Morihno
#El Classico
select g.*, ev.text 
from football.ginf as g
right join football.events as ev on (g.id_odsp=ev.id_odsp)
where g.country like '%spain%' and  g.ht like '%Barcelona%' and g.at like '%Real Madrid%'; 

#Load Data to Pig from Hive
#Note: While loading Hive data into Pig relation, make sure that the user has started Hive metastore service using the below command –
sudo service hive-metastore start
hive –service metastore

Keep the Hive metastore service running in one terminal and use Pig in another terminal
Now to load the hive data into pig, Pig uses HCataLoader() function and it looks like this

pig
grunt > ginfpig = LOAD 'football.ginf' USING org.apache.hive.hcatalog.pig.HCatLoader();
grunt > describe ginfpig;

grunt > eventspig = LOAD 'football.events' USING org.apache.hive.hcatalog.pig.HCatLoader();
grunt > describe eventspig;

# Word Count with pig 
INSERT OVERWRITE DIRECTORY '/user/cloudera/datatp3/football/comments.txt' ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
select  ev.text 
from football.ginf as g
right join football.events as ev on (g.id_odsp=ev.id_odsp)
where g.country like '%spain%' and  g.ht like '%Barcelona%' and g.at like '%Real Madrid%'; 

#comments = LOAD '/user/cloudera/datatp3/football/comments.txt/000001_0' using PigStorage('\t') as (line:chararray);
#words = FOREACH comments GENERATE FLATTEN(TOKENIZE(line)) as word;
#grouped = GROUP words BY word;
#wordcount = FOREACH grouped GENERATE group, COUNT(words);
#DUMP wordcount;

A = load '/user/cloudera/datatp3/football/comments.txt/000001_0';
B = foreach A generate flatten(TOKENIZE((chararray)$0)) as word;
C = filter B by word matches '\\w+';
D = group C by word;
E = foreach D generate COUNT(C), group;
store E into '/user/cloudera/datatp3/football/comments.txt/output';

A_2 = load '/user/cloudera/datatp3/football/comments.txt/000002_0';
B_2 = foreach A_2 generate flatten(TOKENIZE((chararray)$0)) as word;
C_2 = filter B_2 by word matches '\\w+';
D_2 = group C_2 by word;
E_2 = foreach D_2 generate COUNT(C_2), group;
store E_2 into '/user/cloudera/datatp3/football/comments.txt/output2';
