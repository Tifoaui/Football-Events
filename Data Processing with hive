# Préparation d'un dossier qui va contenir nos données sur hdfs  
cd $HADOOP_HOME
hdfs dfs -ls
hdfs dfs -mkdir /user/cloudera/datatp3

#Football Events 
wget https://www.kaggle.com/secareanualin/football-events/downloads/football-events.zip
unzip football-events.zip
#Copier les données depuis Local Vers hdfs
hdfs dfs -mkdir datatp3
hdfs dfs -mkdir  /user/cloudera/datatp3/football
hdfs dfs -copyFromLocal ginf.csv events.csv dictionary.txt /user/cloudera/.staging
hive

CREATE DATABASE IF NOT EXISTS football;
SHOW DATABASES;

# Création de table events
use football;
create table events (
id_odsp string, 
id_event string,
sort_order int,
time int,
text string,
event_type int,
event_type2 int,
side int, 
event_team string,
opponent string,
player string,
player2 string,
player_in string, 
player_out string,
shot_place int,
shot_outcome int,
is_goal int,
location int,
bodypart int,
assist_method int,
situation int,
fast_break int
);
LOAD DATA INPATH '/user/cloudera/.staging/events.csv' OVERWRITE INTO TABLE football.events;

 # Création de table Odds,

use football; 
create table ginf (
 id_odsp string,
 link_odsp string,
 adv_stats BOOLEAN,
 date date,
 league string,
 season string,
 country string,
 ht string,
 at  string,
 fthg  int,
 ftag int,
 odd_h DOUBLE,
 odd_d DOUBLE,
 odd_a DOUBLE,
 odd_over DOUBLE,
 odd_under DOUBLE,
 odd_bts DOUBLE,
 odd_bts_n DOUBLE
);

LOAD DATA INPATH '/user/cloudera/.staging/ginf.csv' OVERWRITE INTO TABLE football.ginf;
